{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "w_LuQFjlzXwG"
      },
      "outputs": [],
      "source": [
        "dataset_path = 'Replace with path to the Dataset folder'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdtSVxuC7888",
        "outputId": "c59695e0-beaa-4111-8210-14ff1253db2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: jsonargparse in /usr/local/lib/python3.10/dist-packages (4.27.1)\n",
            "Requirement already satisfied: catboost in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.35.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.19.4)\n",
            "Requirement already satisfied: PyYAML>=3.13 in /usr/local/lib/python3.10/dist-packages (from jsonargparse) (6.0.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.5.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.3.post1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.11.17)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric sentence_transformers jsonargparse catboost seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHOo6ZGovhDP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch_geometric.nn import GraphSAGE, GCNConv, GATConv, RGATConv, SAGEConv, GINConv, GATv2Conv, CuGraphSAGEConv, to_hetero\n",
        "from torch_geometric.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define training and test paths as Path objects\n",
        "path_to_training = Path(dataset_path + '/training')\n",
        "path_to_test = Path(dataset_path + '/test')\n",
        "\n",
        "# Set the seed for NumPy\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set the seed for PyTorch\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# If you using CUDA, set the seed for it\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "lm_model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "print (\"module %s loaded\")\n",
        "\n",
        "# Define Bert Encoder\n",
        "bert = SentenceTransformer('all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eA2GjSnwLCo"
      },
      "outputs": [],
      "source": [
        "## Define helper functions\n",
        "\n",
        "def embed(input):\n",
        "  return use_model(input)\n",
        "\n",
        "def remove_stop_words(input_list):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_list = []\n",
        "\n",
        "    for sentence in input_list:\n",
        "        # Tokenize the sentence into words\n",
        "        words = nltk.word_tokenize(sentence)\n",
        "\n",
        "        # Remove stop words\n",
        "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "        # Join the filtered words back into a sentence\n",
        "        filtered_sentence = ' '.join(filtered_words)\n",
        "\n",
        "        # Append the filtered sentence to the result list\n",
        "        filtered_list.append(filtered_sentence)\n",
        "\n",
        "    return filtered_list\n",
        "\n",
        "def flatten(list_of_list):\n",
        "    return [item for sublist in list_of_list for item in sublist]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNas9MhCv-zd"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xuj1J4hZv8uv"
      },
      "outputs": [],
      "source": [
        "## Read the JSON files\n",
        "\n",
        "#####\n",
        "# training and test sets of transcription ids\n",
        "#####\n",
        "training_set = ['ES2002', 'ES2005', 'ES2006', 'ES2007', 'ES2008', 'ES2009', 'ES2010', 'ES2012', 'ES2013', 'ES2015', 'ES2016', 'IS1000', 'IS1001', 'IS1002', 'IS1003', 'IS1004', 'IS1005', 'IS1006', 'IS1007', 'TS3005', 'TS3008', 'TS3009', 'TS3010', 'TS3011', 'TS3012']\n",
        "training_set = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in training_set])\n",
        "training_set.remove('IS1002a')\n",
        "training_set.remove('IS1005d')\n",
        "training_set.remove('TS3012c')\n",
        "\n",
        "test_set = ['ES2003', 'ES2004', 'ES2011', 'ES2014', 'IS1008', 'IS1009', 'TS3003', 'TS3004', 'TS3006', 'TS3007']\n",
        "test_set = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in test_set])\n",
        "\n",
        "\n",
        "## Training data\n",
        "y_training = []\n",
        "with open(dataset_path +  \"/training_labels.json\", \"r\") as file:\n",
        "    training_labels = json.load(file)\n",
        "\n",
        "X_training = []\n",
        "for transcription_id in training_set:\n",
        "    with open(path_to_training / f\"{transcription_id}.json\", \"r\") as file:\n",
        "        transcription = json.load(file)\n",
        "\n",
        "    # Encode the speaker within the text itself\n",
        "    for utterance in transcription:\n",
        "        X_training.append(utterance[\"text\"])\n",
        "\n",
        "    y_training += training_labels[transcription_id]\n",
        "\n",
        "## Test data\n",
        "X_test = []\n",
        "cnt = 0\n",
        "idx = []\n",
        "for transcription_id in test_set:\n",
        "    with open(path_to_test / f\"{transcription_id}.json\", \"r\") as file:\n",
        "        transcription = json.load(file)\n",
        "\n",
        "    for utterance in transcription:\n",
        "        X_test.append(utterance[\"text\"])\n",
        "        cnt+=1\n",
        "    idx.append(cnt)\n",
        "    cnt=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipi3IZCpz_jt"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkRZuf5H0Aeg"
      },
      "outputs": [],
      "source": [
        "## Remove <vocalsound>\n",
        "\n",
        "for i in range(len(X_training)):\n",
        "  X_training[i] = X_training[i].replace('<vocalsound> ','')\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "  X_test[i] = X_test[i].replace('<vocalsound>','')\n",
        "\n",
        "## Remove stop words\n",
        "X_training = remove_stop_words(X_training)\n",
        "X_test = remove_stop_words(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EknOcnna0OrN"
      },
      "source": [
        "## Sentence embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FrGbQ3M0Myq"
      },
      "outputs": [],
      "source": [
        "X_training = bert.encode(X_training, show_progress_bar=True)\n",
        "X_test = bert.encode(X_test, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwQGxwpB0Vlf"
      },
      "source": [
        "## Normalize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUCHGZ0f0UkP"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_training_scaled = scaler.fit_transform(X_training)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huQHcj860yPX"
      },
      "source": [
        "## Train / validation split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T97ii2X-01R8"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_training_scaled, y_training, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the shuffled training set into training and validation sets\n",
        "train_size = int(0.8 * len(X_training))\n",
        "train_set_size = int(0.8 * len(training_set))\n",
        "training_set_train = training_set[:train_set_size]\n",
        "training_set_val = training_set[train_set_size:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlR2-pOO09sN"
      },
      "source": [
        "## Knowledge Graph construction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4yrcdRG1Egd"
      },
      "source": [
        "### Compute relationships"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygcz_20i1ATI"
      },
      "outputs": [],
      "source": [
        "def get_relationships(path, training_set):\n",
        "  relationships = []\n",
        "  for transcription_id in training_set:\n",
        "      with open(path / f\"{transcription_id}.txt\", \"r\") as file:\n",
        "          lines = file.readlines()\n",
        "      relationships.append([line.strip().split() for line in lines])\n",
        "\n",
        "  return flatten(relationships)\n",
        "\n",
        "def get_types(relationships):\n",
        "  types = list(set([edge[1] for edge in relationships]))\n",
        "  type_to_number = {edge_type: number for number, edge_type in enumerate(types)}\n",
        "  return types, type_to_number\n",
        "\n",
        "relationships_X_training = get_relationships(path_to_training, training_set) # Whole data\n",
        "relationships_X_train = get_relationships(path_to_training, training_set_train) # Sub train\n",
        "relationships_X_val = get_relationships(path_to_training, training_set_val) # validation\n",
        "\n",
        "types, _ = get_types(relationships_X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnZzgXTS1HOo"
      },
      "source": [
        "### Construct Graph\n",
        "```\n",
        "Node --> Sentence\n",
        "Edge --> Sentences relationships to each other\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iK8uvD_q1JIc"
      },
      "outputs": [],
      "source": [
        "def construct_graph(X, relationships, types, y=[], test = False):\n",
        "  # Get edge indices\n",
        "  edge_indices = {edge_type: [] for edge_type in types}\n",
        "  for type in types:\n",
        "      for edge in relationships:\n",
        "          source, edge_type, target = edge\n",
        "          if edge_type == type:\n",
        "              edge_indices[type].append(torch.tensor([int(source), int(target)], dtype=torch.long).unsqueeze(dim=1))\n",
        "      edge_indices[type] = torch.cat(edge_indices[type], dim=1)\n",
        "\n",
        "  # HeteroData for different types of relationships\n",
        "  data = HeteroData()\n",
        "\n",
        "  data[\"sentence\"].x = torch.tensor(X,  dtype=torch.float32)\n",
        "  if not test:\n",
        "    data[\"sentence\"].y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "  for type in types:\n",
        "      data[\"sentence\", type, \"sentence\"].edge_index = edge_indices[type]\n",
        "  return data\n",
        "\n",
        "## Whole Data\n",
        "data = construct_graph(X_training_scaled, relationships_X_training, get_types(relationships_X_training)[0], y_training)\n",
        "## Use GPU\n",
        "data = data.to('cuda:0')\n",
        "\n",
        "## Train data\n",
        "data_train = construct_graph(X_train, relationships_X_train, get_types(relationships_X_train)[0], y_train)\n",
        "data_train = data_train.to(\"cuda:0\")\n",
        "\n",
        "## Validation Data\n",
        "data_val = construct_graph(X_val, relationships_X_val, get_types(relationships_X_val)[0], y_val)\n",
        "data_val = data_val.to(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SofUAeV1P6n"
      },
      "source": [
        "## GNN Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPjtrEAv1XM_"
      },
      "source": [
        "### GraphSAGE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldCiGn8C1VN6"
      },
      "outputs": [],
      "source": [
        "class GraphSAGEClassifier(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_classes):\n",
        "        super(GraphSAGEClassifier, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels, num_layers=2)\n",
        "        self.conv2 = SAGEConv(hidden_channels, num_classes, num_layers=4)\n",
        "\n",
        "    def forward(self, x, edge_idx):\n",
        "        x = self.conv1(x, edge_idx)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_idx)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy8psDHh1b9c"
      },
      "source": [
        "## GraphGATv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5n2m2DXN1bTE"
      },
      "outputs": [],
      "source": [
        "class GraphGATv2Classifier(torch.nn.Module):\n",
        "  def __init__(self, in_channels, hidden_channels, num_classes):\n",
        "        super(GraphGATv2Classifier, self).__init__()\n",
        "        self.conv1 = GATv2Conv(in_channels, hidden_channels, num_layers=8)\n",
        "        self.conv2 = GATv2Conv(hidden_channels, num_classes, num_layers=4)\n",
        "\n",
        "  def forward(self, x, edge_idx):\n",
        "        x = self.conv1(x, edge_idx)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_idx)\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFWESLb_1m5X"
      },
      "source": [
        "## GIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krTH6w_h1oCQ"
      },
      "outputs": [],
      "source": [
        "class GraphGINClassifier(torch.nn.Module):\n",
        "  def __init__(self, in_channels, hidden_channels, num_classes):\n",
        "        super(GraphGINClassifier, self).__init__()\n",
        "        self.conv1 = GINConv(nn.Sequential(nn.Linear(in_channels, hidden_channels), nn.PReLU()))\n",
        "        self.conv2 = GINConv(nn.Sequential(nn.Linear(hidden_channels, num_classes), nn.PReLU()))\n",
        "\n",
        "  def forward(self, x, edge_idx):\n",
        "        x = self.conv1(x, edge_idx)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_idx)\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FUAK2rY1sl2"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08pF73L71rsn"
      },
      "outputs": [],
      "source": [
        "## Define hyperparameters\n",
        "config = {\"lr\" : 0.0001, \"epochs\" : 2500, \"batch_size\" : 8}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XKP9mkq2BA7"
      },
      "source": [
        "## Definition of train and predict functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RC1BsfZz1u2S"
      },
      "outputs": [],
      "source": [
        "def train_model(model, data):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
        "    # criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    loader = DataLoader([data], batch_size=config[\"batch_size\"], shuffle=True)\n",
        "\n",
        "    # Training loop\n",
        "    for e in tqdm(range(config[\"epochs\"])):\n",
        "        model.train()\n",
        "        train_outs = []\n",
        "        train_targets = []\n",
        "        train_losses = []\n",
        "        for batch in loader:\n",
        "            batch = batch.to(\"cuda:0\")\n",
        "            out = model(batch.x_dict, batch.edge_index_dict)\n",
        "            train_outs.append(out[\"sentence\"])\n",
        "            train_targets.append(batch[\"sentence\"].y.tolist())\n",
        "            loss = F.cross_entropy(\n",
        "                out[\"sentence\"], batch[\"sentence\"].y, torch.tensor([1.0, 5.0], device=\"cuda:0\")\n",
        "            )\n",
        "            print(\"Loss: \", loss)\n",
        "            optimizer.zero_grad()\n",
        "            train_losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    train_targets = np.array(train_targets).squeeze()\n",
        "    train_preds = np.argmax(torch.cat(train_outs).detach().cpu().numpy(), axis=1)\n",
        "\n",
        "    # Calculate F1 score\n",
        "    f1 = f1_score(train_targets, train_preds)\n",
        "\n",
        "    print(\"Train F1 Score:\", f1)\n",
        "\n",
        "\n",
        "def predict(model, data_test):\n",
        "    model.eval()\n",
        "    test_outs = []\n",
        "    test_loader = DataLoader(\n",
        "        [data_test], batch_size=config[\"batch_size\"], shuffle=False\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            test_out = model(batch.x_dict, batch.edge_index_dict)\n",
        "            test_outs.append(test_out[\"sentence\"])\n",
        "\n",
        "    test_preds = np.argmax(torch.cat(test_outs).detach().cpu().numpy(), axis=1)\n",
        "    return test_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsah103G14r9"
      },
      "source": [
        "## Train model on train data only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPT9kPCP18B7"
      },
      "outputs": [],
      "source": [
        "model_SAGE = GraphSAGEClassifier(in_channels=data[\"sentence\"].x.shape[1], hidden_channels=64, num_classes=2)\n",
        "model_SAGE = to_hetero(model_SAGE, data.metadata(), aggr=\"mean\")\n",
        "model_SAGE = model_SAGE.to('cuda:0')\n",
        "\n",
        "model_GATv2 = GraphGATv2Classifier(in_channels=data[\"sentence\"].x.shape[1], hidden_channels=64, num_classes=2)\n",
        "model_GATv2 = to_hetero(model_GATv2, data.metadata(), aggr=\"mean\")\n",
        "model_GATv2 = model_GATv2.to('cuda:0')\n",
        "\n",
        "model_GIN = GraphGINClassifier(in_channels=data[\"sentence\"].x.shape[1], hidden_channels=64, num_classes=2)\n",
        "model_GIN = to_hetero(model_GIN, data.metadata(), aggr=\"mean\")\n",
        "model_GIN = model_GIN.to('cuda:0')\n",
        "\n",
        "train_model(model_SAGE, data_train)\n",
        "train_model(model_GATv2, data_train)\n",
        "train_model(model_GIN, data_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mwvd5Pr2LEd"
      },
      "source": [
        "## Evaluate model on validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGtw4JNc2OPa"
      },
      "outputs": [],
      "source": [
        "# Model Evaluation\n",
        "val_preds_SAGE = predict(model_SAGE, data_val)\n",
        "val_preds_GAT = predict(model_GATv2, data_val)\n",
        "val_preds_GIN = predict(model_GIN, data_val)\n",
        "\n",
        "for i in range(len(X_val)):\n",
        "  if (len(X_val[i]) < 4):\n",
        "    val_preds_SAGE[i] == 0\n",
        "    val_preds_GAT[i] == 0\n",
        "    val_preds_GIN[i] == 0\n",
        "\n",
        "\n",
        "val_preds_vote = np.mean([val_preds_SAGE, val_preds_GAT, val_preds_GIN], axis = 0)\n",
        "val_preds_vote[val_preds_vote >= 0.5] = 1.\n",
        "val_preds_vote[val_preds_vote < 0.5] = 0.\n",
        "\n",
        "\n",
        "# Calculate F1 score for validation dataset\n",
        "f1_SAGE = f1_score(y_val, val_preds_SAGE)\n",
        "f1_GAT = f1_score(y_val, val_preds_GAT)\n",
        "f1_GIN = f1_score(y_val, val_preds_GIN)\n",
        "f1_vote = f1_score(y_val, val_preds_vote)\n",
        "\n",
        "print(\"Validation F1 Score SAGE:\", f1_SAGE)\n",
        "print(\"Validation F1 Score GAT:\", f1_GAT)\n",
        "print(\"Validation F1 Score GIN:\", f1_GIN)\n",
        "print(\"Validation F1 Score Vote:\", f1_vote)\n",
        "\n",
        "conf_matrix = confusion_matrix(y_val, val_preds_vote)\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X644a_j2Pwl"
      },
      "source": [
        "### Main Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLbMZJx12X2G"
      },
      "outputs": [],
      "source": [
        "model_SAGE_main = GraphSAGEClassifier(in_channels=data[\"sentence\"].x.shape[1], hidden_channels=64, num_classes=2)\n",
        "model_SAGE_main = to_hetero(model_SAGE_main, data.metadata(), aggr=\"mean\")\n",
        "model_SAGE_main = model_SAGE.to('cuda:0')\n",
        "\n",
        "model_GAT_main = GraphGATv2Classifier(in_channels=data[\"sentence\"].x.shape[1], hidden_channels=64, num_classes=2)\n",
        "model_GAT_main = to_hetero(model_GAT_main, data.metadata(), aggr=\"mean\")\n",
        "model_GAT_main = model_GATv2.to('cuda:0')\n",
        "\n",
        "model_GIN_main = GraphGINClassifier(in_channels=data[\"sentence\"].x.shape[1], hidden_channels=64, num_classes=2)\n",
        "model_GIN_main = to_hetero(model_GIN_main, data.metadata(), aggr=\"mean\")\n",
        "model_GIN_main = model_GIN.to('cuda:0')\n",
        "\n",
        "train_model(model_SAGE_main, data)\n",
        "train_model(model_GAT_main, data)\n",
        "train_model(model_GIN_main, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkDNAHvF20DV"
      },
      "source": [
        "### Predict test labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8_wgImy20V9"
      },
      "outputs": [],
      "source": [
        "test_relationships = get_relationships(path_to_test, test_set)\n",
        "data_test = construct_graph(X_test_scaled, test_relationships, get_types(test_relationships)[0], test = True)\n",
        "data_test = data_test.to('cuda:0')\n",
        "\n",
        "y_test_SAGE = predict(model_SAGE_main, data_test)\n",
        "y_test_GAT = predict(model_GAT_main, data_test)\n",
        "y_test_GIN = predict(model_GIN_main, data_test)\n",
        "\n",
        "for i in range(len(y_test_SAGE)):\n",
        "  if (len(X_test[i]) < 5):\n",
        "    y_test_SAGE[i] = 0\n",
        "    y_test_GAT[i] = 0\n",
        "    y_test_GIN[i] = 0\n",
        "\n",
        "y_test_vote = np.mean([y_test_SAGE, y_test_GAT, y_test_GIN], axis = 0)\n",
        "y_test_vote[y_test_vote >= 0.5] = 1.\n",
        "y_test_vote[y_test_vote < 0.5] = 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE4hc2i_26dE"
      },
      "source": [
        "### Generate Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGfpuc6S2-6l"
      },
      "outputs": [],
      "source": [
        "test_labels = {}\n",
        "last_idx = 0\n",
        "for i, transcription_id in enumerate(test_set):\n",
        "    test_labels[transcription_id] = y_test_vote[last_idx:last_idx + idx[i]].tolist()\n",
        "    last_idx += idx[i]\n",
        "\n",
        "def make_submission():\n",
        "    file = open(\"submission.csv\", \"w\")\n",
        "    file.write(\"id,target_feature\\n\")\n",
        "    for key, value in test_labels.items():\n",
        "        u_id = [key + \"_\" + str(i) for i in range(len(value))]\n",
        "        target = map(str, value)\n",
        "        for row in zip(u_id, target):\n",
        "            file.write(\",\".join(row))\n",
        "            file.write(\"\\n\")\n",
        "    file.close()\n",
        "\n",
        "make_submission()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
